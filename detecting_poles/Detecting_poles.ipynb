{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DfPPQ6ztJhv4"
   },
   "source": [
    "# Detecting poles\n",
    "\n",
    "Reference: [tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html), for [Mask R-CNN](https://arxiv.org/abs/1703.06870). \n",
    "\n",
    "First, we need to install `pycocotools`. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union. \n",
    "\n",
    "(Mounting drive to access data easily, otherwise upload your own data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using this notebook, make sure to first run, this in shell:\n",
    "``` \n",
    "git clone https://github.com/cocodataset/cocoapi.git\n",
    "cd cocoapi/PythonAPI\n",
    "python setup.py build_ext install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pytorch_kernel` to have all of the conda environment and the useful packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be defined through with these lines: \n",
    "\n",
    "``` $ conda install ipykernel ``` \n",
    "\n",
    "``` $ ipython kernel install --user --name=pytorch_kernel ``` \n",
    "\n",
    "``` $ conda deactivate ``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Sd4jlGp2eLm"
   },
   "source": [
    "## Defining the Dataset\n",
    "\n",
    "The [torchvision reference scripts for training object detection, instance segmentation and person keypoint detection](https://github.com/pytorch/vision/tree/v0.3.0/references/detection) allows for easily supporting adding new custom datasets.\n",
    "The dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n",
    "\n",
    "The only specificity that we require is that the dataset `__getitem__` should return:\n",
    "\n",
    "* image: a PIL Image of size (H, W)\n",
    "* target: a dict containing the following fields\n",
    "    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n",
    "    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n",
    "    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
    "    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes. --> Not used here\n",
    "    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation. We set them all to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bX0rqK-A3Nbl"
   },
   "source": [
    "### Writing a custom dataset for the Airport dataset\n",
    "\n",
    "Let's write a dataset for the Airport dataset. Make sure the data is contained in the dataset folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfwuU-jI3j93"
   },
   "source": [
    "Let's have a look at the dataset and how it is layed down.\n",
    "\n",
    "The data is structured as follows\n",
    "```\n",
    "dataset_aeroport/\n",
    "  annotations/\n",
    "    Image0000.xml\n",
    "    Image0001.xml\n",
    "    Image0002.xml\n",
    "    Image0003.xml\n",
    "    ...\n",
    "  images/\n",
    "    Image0000.jpg\n",
    "    Image0001.jpg\n",
    "    Image0002.jpg\n",
    "    Image0003.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hExyEzYZxdss"
   },
   "source": [
    "#### Some useful functions for bounding boxes representation and xml extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cFHKCvCTxiff"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import argparse\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from imutils.object_detection import non_max_suppression\n",
    "\n",
    "\n",
    "def extract_coordinates(file, min_width=20, img_width=4104):\n",
    "    \"\"\" Extract [xmin, ymin, xmax, ymax] coordinates \"\"\"\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    boxes = []\n",
    "    for bb in root.iter('bndbox'):\n",
    "        x0 = int(bb[0].text)  # xmin\n",
    "        y1 = int(bb[3].text)  # ymax\n",
    "\n",
    "        x1 = int(bb[2].text)  # xmax\n",
    "        y0 = int(bb[1].text)  # ymin\n",
    "        xmin = min(x0, x1)\n",
    "        xmax = max(x0, x1)\n",
    "        if xmax - xmin < min_width:\n",
    "            if xmin + min_width < img_width:\n",
    "                xmax = xmin + min_width\n",
    "            else:\n",
    "                xmin = xmax - min_width\n",
    "        ymin = min(y0, y1)\n",
    "        ymax = max(y0, y1)\n",
    "        if ymax - ymin < min_width:\n",
    "            ymax = ymin + min_width\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def export_xml(file):\n",
    "    \"\"\" Export xml file to drax bounding box in OpenCV \"\"\"\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    corners_up_left = []\n",
    "    corners_bottom_left = []\n",
    "    corner_up_right = []\n",
    "    corners_bottom_right = []\n",
    "    for bb in root.iter('bndbox'):\n",
    "        x1 = int(bb[0].text)  # xmin\n",
    "        x2 = int(bb[3].text)  # ymax\n",
    "        corners_up_left.append((x1, x2))\n",
    "        y1 = int(bb[2].text)  # xmax\n",
    "        y2 = int(bb[1].text)  # ymin\n",
    "        corners_bottom_right.append((y1, y2))\n",
    "    return corners_up_left, corners_bottom_right\n",
    "    # return corners_bottom_right, corners_up_left\n",
    "\n",
    "def draw_bounding_box(image, corners_up_left, corners_bottom_right):\n",
    "    \"\"\" draw bounding boxes given the corners [xmin, ymax] and [xmax, ymin]\"\"\"\n",
    "    for i in range(len(corners_up_left)):\n",
    "        x = corners_up_left[i]\n",
    "        y = corners_bottom_right[i]  # x and y are two opposite corners of the rectangle\n",
    "        cv.rectangle(image, x, y, (255, 255, 255), thickness=10)  # (0, 255, 0), 2)\n",
    "    plt.figure()\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "def display_image_with_bounding_boxes(image, target, to_npy=True):\n",
    "    \"\"\" Display image with bounding boxes given image and target from dataset \"\"\"\n",
    "    if to_npy:\n",
    "        image = image.numpy()\n",
    "    image = np.sum(image, 0)\n",
    "\n",
    "    # bounding boxes\n",
    "    boxes = target['boxes']\n",
    "    boxes = boxes.cpu()\n",
    "    if type(boxes) != np.ndarray:\n",
    "          boxes = boxes.numpy()\n",
    "    num_boxes = boxes.shape[0]\n",
    "    corners_up_left = [(boxes[i][0], boxes[i][3]) for i in range(num_boxes)]\n",
    "    corners_bottom_right = [(boxes[i][2], boxes[i][1]) for i in range(num_boxes)]\n",
    "    draw_bounding_box(image, corners_up_left, corners_bottom_right)\n",
    "\n",
    "def PIL_disp_img_bb(image, target, color='red'):\n",
    "    to_pil = torchvision.transforms.ToPILImage(mode='RGB')\n",
    "    base = to_pil(image)\n",
    "    base = base.convert('RGBA')\n",
    "    box_container = Image.new(base.mode, base.size, (255,255,255, 0))\n",
    "    d = ImageDraw.Draw(box_container)\n",
    "    boxes = target['boxes']\n",
    "    boxes = boxes.cpu()\n",
    "    if type(boxes) != np.ndarray:\n",
    "        boxes = boxes.numpy()\n",
    "    num_boxes = boxes.shape[0]\n",
    "    for i in range(num_boxes):\n",
    "        bb = d.rectangle(boxes[i], outline=color)\n",
    "    base.show()\n",
    "    box_container.show()\n",
    "    # return base, box_container\n",
    "    out = Image.alpha_composite(base, box_container)\n",
    "    return out\n",
    "\n",
    "SMOOTH = 1e-6\n",
    "\n",
    "def iou_pytorch(img, annotated_bb, gt_bb):\n",
    "    img_size = img.size()\n",
    "    outputs = torch.zeros((img_size[2], img_size[1]), dtype = int)\n",
    "    labels = torch.zeros((img_size[2], img_size[1]), dtype = int)\n",
    "    for bb in annotated_bb.tolist():\n",
    "        for x in range(int(bb[0]), int(bb[2])):\n",
    "            for y in range(int(bb[1]), int(bb[3])):\n",
    "                outputs[x, y] = 1\n",
    "    for bb in gt_bb.tolist():\n",
    "        for x in range(int(bb[0]), int(bb[2])):\n",
    "            for y in range(int(bb[1]), int(bb[3])):\n",
    "                labels[x, y] = 1\n",
    "    \n",
    "    intersection = (outputs & labels).float().sum()  # sum((1, 2)) Will be zero if Truth=0 or Prediction=0\n",
    "    union = (outputs | labels).float().sum()         # sum((1, 2)) Will be zzero if both are 0\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "    \n",
    "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresholds\n",
    "    \n",
    "    return thresholded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9Ee5NV54Dmj"
   },
   "source": [
    "So each image has a corresponding set of bounding boxes. Let's write a `torch.utils.data.Dataset` class for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTgWtixZTs3X"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class AirportDataSet(object):\n",
    "    def __init__(self, root_dir, transforms):\n",
    "        self.root = root_dir\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to ensure that they are aligned\n",
    "        self.images = list(sorted(os.listdir(os.path.join(root_dir, \"images\"))))\n",
    "        self.bb = list(sorted(os.listdir(os.path.join(root_dir, \"annotations\"))))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load images and bounding boxes\n",
    "        image_path = os.path.join(self.root, \"images\", self.images[index])\n",
    "        bb_path = os.path.join(self.root, \"annotations\", self.bb[index])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        width, height = image.size\n",
    "\n",
    "        boxes = extract_coordinates(bb_path, img_width=width)\n",
    "\n",
    "        # convert everything into a tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((len(boxes), ), dtype=torch.int64)\n",
    "\n",
    "        # image_id = torch.tensor([index])\n",
    "        # area = (boxes[:, 3] - boxes[:, 1])*(boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        # target['area'] = area\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.as_tensor(index, dtype = torch.int64)\n",
    "        target['area'] = torch.zeros((len(boxes), ), dtype=torch.float32)\n",
    "        target['iscrowd'] = torch.zeros((len(boxes), ), dtype=torch.int64)\n",
    "\n",
    "        # compute areas of boxes\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            target['area'][i] = (box[2]-box[0])*(box[3]-box[1]) \n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "            # transform target\n",
    "            # boxes[:, 0] = width - boxes[:, 0]  # x_min\n",
    "            # boxes[:, 2] = width - boxes[:, 2]  # x_max\n",
    "            # target['boxes'] = boxes\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RoAEkUgn4uEq"
   },
   "source": [
    "## Defining your model\n",
    "\n",
    "In this tutorial, we will be using [Faster R-CNN](https://arxiv.org/abs/1506.01497). Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.\n",
    "\n",
    "We use a pretrained (on COCO) from the `torchvision.models` library, only modifying the number of possible classes to 2 (pole or not pole). \n",
    "\n",
    "Another solution (for faster predictions especially) would be to modify the backbone (with MobileNetv2 from the same library). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSQIGmCm3Cnd"
   },
   "source": [
    "### An Instance segmentation model for the Airport Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjNHjVMOyYlH"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "      \n",
    "def get_instance_segmentation_model(num_classes, model='mobilenet'):\n",
    "    if model == 'faster-rcnn':\n",
    "        # model = torch.load('/content/drive/My Drive/CNN_trainings/detecting_poles.pth')\n",
    "      # load an instance segmentation model pre-trained on COCO -- uncomment to train faster rcnn\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, progress=False)\n",
    "\n",
    "      # get the number of input features for the classifier\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "      # replace the pre-trained head with a new one\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    else:\n",
    "        backbone = torchvision.models.mobilenet_v2(pretrained=True, progress=False).features\n",
    "        backbone.out_channels = 1280\n",
    "        anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "        roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
    "        model = FasterRCNN(backbone, num_classes=2, rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-WXLwePV5ieP"
   },
   "source": [
    "## Training and evaluation functions\n",
    "\n",
    "In `references/detection/`from the pytorch git, there are a number of helper functions to simplify training and evaluating detection models.\n",
    "Here, we will use `references/detection/engine.py`, `references/detection/utils.py` and `references/detection/transforms.py`.\n",
    "\n",
    "Let's copy those files (and their dependencies) in here so that they are available in the notebook, run these lines in the terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5498,
     "status": "ok",
     "timestamp": 1583937598677,
     "user": {
      "displayName": "Capucine NGHIEM",
      "photoUrl": "",
      "userId": "17742733405569528643"
     },
     "user_tz": -60
    },
    "id": "UYDb7PBw55b-",
    "outputId": "33d7fb79-b302-4318-9f3f-c6b42334636b"
   },
   "source": [
    "```\n",
    "git clone https://github.com/pytorch/vision.git\n",
    "cd vision\n",
    "git checkout v0.3.0\n",
    "\n",
    "cp references/detection/utils.py ../\n",
    "cp references/detection/transforms.py ../\n",
    "cp references/detection/coco_eval.py ../\n",
    "cp references/detection/engine.py ../\n",
    "cp references/detection/coco_utils.py ../\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2u9e_pdv54nG"
   },
   "source": [
    "\n",
    "\n",
    "Let's write some helper functions for data augmentation / transformation, which leverages the functions in `references/detection` that we have just copied:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l79ivkwKy357"
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzCLqiZk-sjf"
   },
   "source": [
    "#### Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YFJGJxk6XEs"
   },
   "source": [
    "## Training Faster RCNN\n",
    "\n",
    "We now have the dataset class, the models and the data transforms. Let's instantiate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5dGaIezze3y"
   },
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = AirportDataSet('dataset/dataset_aeroport', get_transform(train=True))\n",
    "dataset_test = AirportDataSet('dataset/dataset_aeroport', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])  #[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])  #[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "print(\"Size of the dataset: \\t\", len(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5yvZUprj4ZN"
   },
   "source": [
    "Now let's instantiate the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoenkCj18C4h"
   },
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes, 'faster-rcnn')\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAd56lt4kDxc"
   },
   "source": [
    "And now let's train the model for 10 epochs, evaluating at the end of every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 352214,
     "status": "ok",
     "timestamp": 1582124320725,
     "user": {
      "displayName": "Capucine NGHIEM",
      "photoUrl": "",
      "userId": "17742733405569528643"
     },
     "user_tz": -60
    },
    "id": "at-h4OWK0aoc",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "86982019-ff69-4092-fe7c-8bc46ce123d9"
   },
   "outputs": [],
   "source": [
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6mYGFLxkO8F"
   },
   "source": [
    "Now that training has finished, let's have a look at what it actually predicts in a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHwIdxH76uPj"
   },
   "outputs": [],
   "source": [
    "# pick one image from the set\n",
    "index = 1\n",
    "# plt_img = plt.imread('/content/experiment1/images/Image0000.jpg')\n",
    "img, _ = dataset_test[index]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DmN602iKsuey"
   },
   "source": [
    "Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\n",
    "The dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, `masks` and `scores` as fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 554,
     "status": "ok",
     "timestamp": 1581513607977,
     "user": {
      "displayName": "Capucine NGHIEM",
      "photoUrl": "",
      "userId": "17742733405569528643"
     },
     "user_tz": -60
    },
    "id": "Lkmb3qUu6zw3",
    "outputId": "71d19f6c-6697-4d61-8e48-b1fc47b94713"
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwT21rzotFbH"
   },
   "source": [
    "Let's inspect the image and the predicted segmentation masks.\n",
    "\n",
    "For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqIYw3sylnwk"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'CNN_trainings/faster-rcnn_weights.pth')\n",
    "torch.save(model, 'CNN_trainings/faster-rcnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean accuracy (compared to man made ground truth)\n",
    "dataset = AirportDataSet('dataset/dataset_aeroport', get_transform(train=True))\n",
    "model.eval()\n",
    "acc = np.zeros(len(dataset))\n",
    "for index in range(len(dataset)):\n",
    "    img, target = dataset[index]\n",
    "    img = pil2tensor(img)\n",
    "    if index % 10 == 0:\n",
    "        print(\"Annotating frame \\t\", index)\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])\n",
    "    # out = PIL_disp_img_bb(img, pred[0])\n",
    "    # out.save('results/aeroport1/annotated_'+str(index)+'.png')\n",
    "    acc[index] = iou_pytorch(img, pred[0]['boxes'], target['boxes'])\n",
    "print(\"Mean accuracy of Faster-RCNN on Airport dataset: \\n\", np.mean(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M58J3O9OtT1G"
   },
   "source": [
    "And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5v5S3bm07SO1"
   },
   "outputs": [],
   "source": [
    "out = PIL_disp_img_bb(img, prediction[0])\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ys4PdCc24SYM"
   },
   "source": [
    "## Training MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1012,
     "status": "ok",
     "timestamp": 1583939097606,
     "user": {
      "displayName": "Capucine NGHIEM",
      "photoUrl": "",
      "userId": "17742733405569528643"
     },
     "user_tz": -60
    },
    "id": "SdaRDsUT4Y05",
    "outputId": "cce4aadc-debb-4eb7-b6f3-67cde9ed3c65"
   },
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = AirportDataSet('dataset/dataset_aeroport', get_transform(train=True))\n",
    "dataset_test = AirportDataSet('dataset/dataset_aeroport', get_transform(train=False))\n",
    "print(\"Dataset size: \", len(dataset.images)-50)\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])  \n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])  \n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1987,
     "status": "error",
     "timestamp": 1583939122456,
     "user": {
      "displayName": "Capucine NGHIEM",
      "photoUrl": "",
      "userId": "17742733405569528643"
     },
     "user_tz": -60
    },
    "id": "NWtFp4nF4e-R",
    "outputId": "ae801dbc-704d-4338-c0f7-8c1e139c6924"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9687,
     "status": "error",
     "timestamp": 1583939112310,
     "user": {
      "displayName": "Capucine NGHIEM",
      "photoUrl": "",
      "userId": "17742733405569528643"
     },
     "user_tz": -60
    },
    "id": "zh0t8Cb94l00",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "58831481-479f-48eb-b11a-9d0b9e03ba14"
   },
   "outputs": [],
   "source": [
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2nTXku74zYP"
   },
   "outputs": [],
   "source": [
    "# pick one image from the set\n",
    "index = 1\n",
    "# plt_img = plt.imread('/content/experiment1/images/Image0000.jpg')\n",
    "img, _ = dataset_test[index]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean accuracy (compared to man made ground truth)\n",
    "pil2tensor = torchvision.transforms.ToTensor()\n",
    "dataset = AirportDataSet('dataset/dataset_aeroport', get_transform(train=False))\n",
    "model.eval()\n",
    "acc = np.zeros(len(dataset))\n",
    "for index in range(len(dataset)):\n",
    "    img, target = dataset[index]\n",
    "    # img = pil2tensor(img)\n",
    "    if index % 10 == 0:\n",
    "        print(\"Annotating frame \\t\", index)\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])\n",
    "    out = PIL_disp_img_bb(img, prediction[0])\n",
    "    out.save('results/aeroport1_ssd/annotated_'+str(index)+'.png')\n",
    "    # acc[index] = iou_pytorch(img, prediction[0]['boxes'], target['boxes'])\n",
    "# print(\"Mean accuracy of MobileNet on Airport dataset: \\n\", np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 276702,
     "status": "error",
     "timestamp": 1583334724625,
     "user": {
      "displayName": "Capucine NGHIEM",
      "photoUrl": "",
      "userId": "17742733405569528643"
     },
     "user_tz": -60
    },
    "id": "nWKx6rOJ40SH",
    "outputId": "f74d4406-c124-45bb-deb1-e9d93a0936e9"
   },
   "outputs": [],
   "source": [
    "# saving the model/the weights\n",
    "torch.save(model.state_dict(), 'CNN_trainings/mobile_detection_weights.pth')\n",
    "torch.save(model, 'CNN_trainings/mobile_detection.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZGG8tMB49Pz"
   },
   "outputs": [],
   "source": [
    "# visualizing a result\n",
    "out = PIL_disp_img_bb(img, prediction[0])\n",
    "out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KQF0R2M9Mvg"
   },
   "source": [
    "## Testing on new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfY_yqV7Cq36"
   },
   "outputs": [],
   "source": [
    "# only execute if no training before (warning only Faster RCNN model is available) and upload your test archive\n",
    "model = torch.load('CNN_trainings/faster-rcnn.pth')\n",
    "# model = torch.load('CNN_trainings/mobile_detection.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1550,
     "status": "ok",
     "timestamp": 1583338522897,
     "user": {
      "displayName": "Capucine NGHIEM",
      "photoUrl": "",
      "userId": "17742733405569528643"
     },
     "user_tz": -60
    },
    "id": "kk5q6die4buB",
    "outputId": "980aab1f-2849-473a-b0e0-aefbb3f0d6da"
   },
   "outputs": [],
   "source": [
    "# test on another image\n",
    "# CEA dataset\n",
    "test_set = AirportDataSet('dataset/cea_dataset', None)\n",
    "# airport dataset (1)\n",
    "# test_set = AirportDataSet('dataset/dataset_aeroport', None)\n",
    "# airport dataset (2)\n",
    "# test_set = AirportDataSet('dataset/aeroport2', None)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "index = 2\n",
    "test_img, _ = test_set[index]\n",
    "target = _\n",
    "pil2tensor = torchvision.transforms.ToTensor()\n",
    "test_img = pil2tensor(test_img)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model([test_img.to(device)])\n",
    "display_image_with_bounding_boxes(test_img, pred[0])\n",
    "print(pred[0]['scores'])\n",
    "print('# bounding boxes found for img ', index, ': ', len(pred[0]['boxes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dLZKnUpL9iJZ"
   },
   "outputs": [],
   "source": [
    "def apply_NMS(pred, filter=True, proba_confidence=0.2):\n",
    "    bounding_boxes = pred[0]['boxes'].cpu().numpy()\n",
    "    scores = pred[0]['scores'].cpu().numpy()\n",
    "    if filter:\n",
    "        filter_to_apply = np.zeros(bounding_boxes.shape[0], dtype=bool)\n",
    "        for i in range(bounding_boxes.shape[0]):\n",
    "            bb = bounding_boxes[i]\n",
    "            filter_to_apply[i] = bb[2] - bb[0] > 10 and bb[3] - bb[1] > 2*(bb[2]-bb[0]) and scores[i]>=proba_confidence\n",
    "        bounding_boxes = bounding_boxes[filter_to_apply]\n",
    "        scores = scores[filter_to_apply]\n",
    "\n",
    "    # bounding_boxes = non_max_suppression(bounding_boxes, overlapThresh=0.1)\n",
    "    pred[0]['boxes'] = torch.Tensor(bounding_boxes) \n",
    "    pred[0]['scores'] = torch.Tensor(scores)  # no NMS to use this line\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1177,
     "status": "ok",
     "timestamp": 1583338612615,
     "user": {
      "displayName": "Capucine NGHIEM",
      "photoUrl": "",
      "userId": "17742733405569528643"
     },
     "user_tz": -60
    },
    "id": "XhHIGnPExR9g",
    "outputId": "073a2c0a-20d1-4824-e370-ae3182f5145e"
   },
   "outputs": [],
   "source": [
    "# testing NMS (Normal Maximum Suppression)\n",
    "pred_= apply_NMS(pred, proba_confidence = 0.15)\n",
    "display_image_with_bounding_boxes(test_img, pred_[0])\n",
    "print('Scores: ', pred_[0]['scores'])\n",
    "print('# bounding boxes kept after NMS: ', len(pred_[0]['boxes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bt0xILRaFliL"
   },
   "outputs": [],
   "source": [
    "pil2tensor = torchvision.transforms.ToTensor()\n",
    "acc = np.zeros(len(test_set))\n",
    "model.eval()\n",
    "for index in range(len(test_set)):\n",
    "    test_img, _ = test_set[index]\n",
    "    target = _\n",
    "    if index % 10 == 0:\n",
    "        print(\"Annotating frame \\t\", index)\n",
    "    test_img = pil2tensor(test_img)\n",
    "    with torch.no_grad():\n",
    "        pred = model([test_img.to(device)])\n",
    "        # pred = apply_NMS(pred)\n",
    "    out = PIL_disp_img_bb(test_img, pred[0])\n",
    "    out.save('results/cea/annotated_'+str(index)+'.png')\n",
    "    # acc[index] = iou_pytorch(test_img, pred[0]['boxes'], target['boxes'])\n",
    "# print(\"Mean accuracy: \\n\", np.mean(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7GHXnRs34DEA"
   },
   "source": [
    "All results can be consulted in the folder `results` following the path above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVFHMccL-b5H"
   },
   "outputs": [],
   "source": [
    "out = PIL_disp_img_bb(test_img, pred[0])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_with_bounding_boxes(test_img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 545,
     "status": "error",
     "timestamp": 1582122933310,
     "user": {
      "displayName": "Capucine NGHIEM",
      "photoUrl": "",
      "userId": "17742733405569528643"
     },
     "user_tz": -60
    },
    "id": "jQl2JRLy5gR3",
    "outputId": "cf522115-8650-41da-a65f-704812c98889"
   },
   "outputs": [],
   "source": [
    "true = PIL_disp_img_bb(test_img, target, color=(0, 255, 0))\n",
    "# true"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "z2WrepRp23yu",
    "-WXLwePV5ieP",
    "FzCLqiZk-sjf",
    "3YFJGJxk6XEs"
   ],
   "name": "Detecting_poles.ipynb",
   "provenance": [
    {
     "file_id": "1gsN450yskLwSPB9BI4ifKY20iJ_qZ4Zq",
     "timestamp": 1579105859248
    },
    {
     "file_id": "https://github.com/pytorch/vision/blob/temp-tutorial/tutorials/torchvision_finetuning_instance_segmentation.ipynb",
     "timestamp": 1579100188893
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
